{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Naive Bayes classifier\n",
    "Naive Bayes is a straightforward example to help learn the Bayesian probabilistic perspective.\n",
    "\n",
    "I will first run a simple toy example and examine the implementation with the scikit learn.\n",
    "\n",
    "[TODO: use scikit learn for more complicated examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example: vegetarian buying habit\n",
    "Imagining you are running an online grocery shopping site and you would like to send an email about amazing vegetarian ready-meals. A simple way is to predict using their items bought. You've got some training data about the vegetarian (y = 1) and non-vegterian (y=0) customers. Let us start simple, by including only two features: (1) whether they bought Quorn before $x_1$ and (2) whether they bought steak before $x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 20.  70.]\n",
      "[ 80.  10.]\n"
     ]
    }
   ],
   "source": [
    "# Create training data\n",
    "# 200 Users, first 100 are non-vegetarian\n",
    "X = np.zeros((200,2))\n",
    "Y = np.zeros((200))\n",
    "Y[100:] = 1\n",
    "# Randomly assign features\n",
    "rand_idx = np.random.permutation(100)\n",
    "X[rand_idx[:20],0] = 1\n",
    "rand_idx = np.random.permutation(100)\n",
    "X[rand_idx[:70],1] = 1\n",
    "rand_idx = np.random.permutation(100)+100\n",
    "X[rand_idx[:80],0] = 1\n",
    "rand_idx = np.random.permutation(100)+100\n",
    "X[rand_idx[:10],1] = 1\n",
    "print(sum(X[:100,:]))\n",
    "print(sum(X[100:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian classifier\n",
    "According to how the data is generated, the Bernoulli probability distribution is \n",
    "$$ p(x_1=1|y=0) = 0.2,\\ p(x_1=1|y=1) = 0.8, \\ p(x_2 = 1|y = 0) = 0.7, \\ p(x_2 = 1|y = 1) = 0.1 $$\n",
    "Thus given a new example $x = [1,1]$ The prediction is \n",
    "$$p(y = 0|x = [1,1]) = \\frac{p(x|y)p(y)}{p(x)} = \\frac{p(y=0)p(x1=1|y = 0)p(x2=1|y=0)}{p(y=0)p(x1=1|y = 0)p(x2=1|y=0)+p(y=1)p(x1=1|y = 1)p(x2=1|y=1)}$$\n",
    "The result is\n",
    "$$p(y = 0|x = [1,1]) = \\frac{0.5\\times 0.2 \\times 0.7}{0.5\\times 0.2 \\times 0.7 + 0.5\\times 0.8 \\times 0.1} = 7/11 = 0.636 $$\n",
    "Thus the probability of a user getting both quorn and steak is a non-vegetarian is 0.636."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting the classes:  [ 100.  100.]\n",
      "Counting the features: \n",
      " [[ 20.  70.]\n",
      " [ 80.  10.]]\n",
      "[[ 0.63636364  0.36363636]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meichenlu/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB(alpha=0, binarize=0.0, class_prior=None, fit_prior=True)\n",
    "clf.fit(X, Y)\n",
    "# Check some properties\n",
    "print('Counting the classes: ', clf.class_count_)\n",
    "print('Counting the features: \\n', clf.feature_count_)\n",
    "# Try predicting x= [1,1] (Note x should be a column vector!)\n",
    "new_x = np.ones((1,2))\n",
    "print(clf.predict_proba(new_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This probability (0.636) is what we expect! \n",
    "\n",
    "Let us see how the Laplace smoothing is applied.\n",
    "According to Andrew Ng's notes, the Laplace smoothing is given by \n",
    "$$ p(x_j=1|y=0) = \\frac{\\sum_{i=1}^m 1\\{x_j^{(i)} \\bigwedge y^{(i)}=0 \\} + 1 }{\\sum_{i=1}^m 1\\{y^{(i)}=0 \\} + 2}$$\n",
    "- Symbol $1\\{y^{(i)}=0 \\}$ gives 1 for all $y^{(i)}=0$ and 0 otherwise\n",
    "- the $+2$ at the bottom represents then number of all classes\n",
    "\n",
    "Therefore in this case the adjusted values are\n",
    "$$ p(x_1=1|y=0) = 21/102,\\ p(x_1=1|y=1) = 81/102, \\ p(x_2 = 1|y = 0) = 71/102, \\ p(x_2 = 1|y = 1) = 11/102 $$\n",
    "The new example thus has probability\n",
    "$$p(y = 0|x = [1,1]) = \\frac{0.5\\times \\frac{21}{102} \\times \\frac{71}{102}}{0.5\\times \\frac{21}{102} \\times \\frac{71}{102} + 0.5\\times \\frac{81}{102} \\times \\frac{11}{102}} = 7/11 = 0.626 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.62594458  0.37405542]]\n"
     ]
    }
   ],
   "source": [
    "clf2 = BernoulliNB(alpha=1, binarize=0.0, class_prior=None, fit_prior=True)\n",
    "clf2.fit(X, Y)\n",
    "new_x = np.ones((1,2))\n",
    "print(clf2.predict_proba(new_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we understand what alpha is! What if alpha = $0.5$, we can guess that instead of adding $1$, we add $0.5$ at the top  of the Laplace smoothing (of course the bottom will be adjusted too, in this case be $+1$ instead of $+2$. \n",
    "\n",
    "We can also include some prior information using the parameters fit_prior and class_prior. Let's assume that by default the user is more likely to be non vegetarian with $p(y=1) = 0.4$. The results for the previous scenario becomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.71510791  0.28489209]]\n"
     ]
    }
   ],
   "source": [
    "clf2 = BernoulliNB(alpha=1, binarize=0.0, class_prior=[0.6,0.4], fit_prior=True)\n",
    "clf2.fit(X, Y)\n",
    "new_x = np.ones((1,2))\n",
    "print(clf2.predict_proba(new_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With intuition, the above results is the same as \n",
    "$$p(y = 0|x = [1,1]) = \\frac{0.6 \\times \\frac{21}{102} \\times \\frac{71}{102}}{0.6\\times \\frac{21}{102} \\times \\frac{71}{102} + 0.4\\times \\frac{81}{102} \\times \\frac{11}{102}} = 0.715 $$\n",
    "Therefore, it uses the class_prior we provided. If class_prior is provded the fit_prior is not useful. However, if fit_prior is set to false, the class_prior is assumed [0.5,0.5] instead of following the distribution provided by the data.\n",
    "\n",
    "In principle, we can also use Multinomial Naive Bayes. Let us try this now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.62594458  0.37405542]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_MN = MultinomialNB()\n",
    "clf_MN.fit(X,Y)\n",
    "new_x = np.ones((1,2))\n",
    "print(clf_MN.predict_proba(new_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other examples to explore are\n",
    "- [Fit MNIST](https://github.com/bikz05/ipython-notebooks/blob/master/machine-learning/naive-bayes-mnist-sklearn.ipynb)\n",
    "- [Spam classifier in CS229's problem sheet 3](http://cs229.stanford.edu/syllabus.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
