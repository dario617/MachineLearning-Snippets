{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network\n",
    "Meichen Lu (meichenlu91@gmail.com) 13rd April 2018\n",
    "\n",
    "In this notebook, I will implement a simple 2-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fprop_logistic(x, y, params):\n",
    "  # Follows procedure given in notes\n",
    "    W1, b1, W2, b2 = [params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "    z1 = np.dot(W1, x) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    loss = -(y * np.log(a2) + (1-y) * np.log(1-a2))\n",
    "    ret = {'x': x, 'y': y, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'loss': loss}\n",
    "    for key in params:\n",
    "        ret[key] = params[key]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bprop_logistic(fprop_cache):\n",
    "  # Follows procedure given in notes\n",
    "    x, y, z1, a1, z2, a2, loss = [fprop_cache[key] for key in ('x', 'y', 'z1', 'a1', 'z2', 'a2', 'loss')]\n",
    "    dz2 = (a2 - y)\n",
    "    dW2 = np.dot(dz2, a1.T)\n",
    "    db2 = dz2\n",
    "    dz1 = np.dot(fprop_cache['W2'].T, dz2) * sigmoid(z1) * (1-sigmoid(z1))\n",
    "    dW1 = np.dot(dz1, x.T)\n",
    "    db1 = dz1\n",
    "    return {'b1': db1, 'W1': dW1, 'b2': db2, 'W2': dW2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bprop_logistic_vec(fprop_cache):\n",
    "  # Vectorised version of the bprop_logistic()\n",
    "    x, y, z1, a1, z2, a2, loss = [fprop_cache[key] for key in ('x', 'y', 'z1', 'a1', 'z2', 'a2', 'loss')]\n",
    "    delta2 = (a2 - y)\n",
    "    dW2 = np.dot(delta2, a1.T)\n",
    "    db2 = np.sum(delta2, 1) # Add up contribution from all examples\n",
    "    delta1 = np.dot(fprop_cache['W2'].T, delta2) * sigmoid(z1) * (1-sigmoid(z1))\n",
    "    dW1 = np.dot(delta1, x.T)\n",
    "    db1 = np.sum(delta1,1)\n",
    "    return {'b1': db1, 'W1': dW1, 'b2': db2, 'W2': dW2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_gradient_bprop_logistic(m, n, params):\n",
    "    from copy import copy\n",
    "# Initialize random parameters and inputs\n",
    "    x = np.random.rand(m, n)\n",
    "    y = np.random.randint(0, 2, size=(1,n))  # Returns 0/1\n",
    "\n",
    "    fprop_cache = fprop_logistic(x, y, params)\n",
    "    if n == 1:\n",
    "        print('check stochastic gradient descent')\n",
    "        bprop_cache = bprop_logistic(fprop_cache)\n",
    "    else:\n",
    "        bprop_cache = bprop_logistic_vec(fprop_cache)\n",
    "    \n",
    "    # Numerical gradient checking\n",
    "    # Note how slow this is! Thus we want to use the backpropagation algorithm instead.\n",
    "    eps = 1e-6\n",
    "    ng_cache = {}\n",
    "    # For every single parameter (W, b)\n",
    "    for key in params:\n",
    "        param = params[key]\n",
    "        # This will be our numerical gradient\n",
    "        ng = np.zeros(param.shape)\n",
    "        for j in range(ng.shape[0]):\n",
    "            for k in range(ng.shape[1]):\n",
    "                # For every element of parameter matrix, compute gradient of loss wrt\n",
    "                # that element numerically using finite differences\n",
    "                add_eps = np.copy(param)\n",
    "                min_eps = np.copy(param)\n",
    "                add_eps[j, k] += eps\n",
    "                min_eps[j, k] -= eps\n",
    "                add_params = copy(params)\n",
    "                min_params = copy(params)\n",
    "                add_params[key] = add_eps\n",
    "                min_params[key] = min_eps\n",
    "                ng[j, k] = np.sum(fprop_logistic(x, y, add_params)['loss'] - fprop_logistic(x, y, min_params)['loss']) / (2 * eps)\n",
    "            ng_cache[key] = ng\n",
    "\n",
    "    # Compare numerical gradients to those computed using backpropagation algorithm\n",
    "    for key in params:\n",
    "        print(key)\n",
    "        # These should be the same\n",
    "        print(bprop_cache[key])\n",
    "        print(ng_cache[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialise_param(m, n, n1, n2):\n",
    "    '''\n",
    "    m features, n samples, first layer with n1 cells, second layer with n2 cells\n",
    "    '''\n",
    "    W1 = np.random.rand(n1, m)\n",
    "    b1 = np.random.rand(n1, 1)\n",
    "    W2 = np.random.rand(n2, n1)\n",
    "    b2 = np.random.rand(n2, 1)\n",
    "    params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check stochastic gradient descent\n",
      "W1\n",
      "[[ 0.05899921  0.01094255]\n",
      " [ 0.08199823  0.01520816]\n",
      " [ 0.06211878  0.01152113]]\n",
      "[[ 0.05899921  0.01094255]\n",
      " [ 0.08199823  0.01520816]\n",
      " [ 0.06211878  0.01152113]]\n",
      "b1\n",
      "[[ 0.0984633 ]\n",
      " [ 0.13684618]\n",
      " [ 0.10366952]]\n",
      "[[ 0.0984633 ]\n",
      " [ 0.13684618]\n",
      " [ 0.10366952]]\n",
      "W2\n",
      "[[ 0.61362244  0.53093868  0.58239993]]\n",
      "[[ 0.61362244  0.53093868  0.58239993]]\n",
      "b2\n",
      "[[ 0.83940254]]\n",
      "[[ 0.83940254]]\n"
     ]
    }
   ],
   "source": [
    "m = 2 # features\n",
    "n = 1 # examples\n",
    "params = initialise_param(m, n, n1 = 3, n2 = 1)\n",
    "# Stochastic case\n",
    "numerical_gradient_bprop_logistic(m, n, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1\n",
      "[[ 0.08746158  0.01376536]\n",
      " [ 0.07796684  0.01230863]\n",
      " [ 0.03607751  0.00569313]]\n",
      "[[ 0.08746158  0.01376536]\n",
      " [ 0.07796684  0.01230863]\n",
      " [ 0.03607751  0.00569313]]\n",
      "b1\n",
      "[ 0.15720113  0.13856309  0.06421861]\n",
      "[[ 0.15720113]\n",
      " [ 0.13856309]\n",
      " [ 0.06421861]]\n",
      "W2\n",
      "[[ 1.07937544  0.99306597  1.05064755]]\n",
      "[[ 1.07937544  0.99306597  1.05064755]]\n",
      "b2\n",
      "[ 1.46690793]\n",
      "[[ 1.46690793]]\n"
     ]
    }
   ],
   "source": [
    "m = 2 # features\n",
    "n = 2 # examples\n",
    "params = initialise_param(m, n, n1 = 3, n2 = 1)\n",
    "# Vectorised case\n",
    "numerical_gradient_bprop_logistic(m, n, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net_logistic_stochastic(X_train, Y_train, params, max_iter = 200, alpha = 0.5, verbose = False):\n",
    "    '''\n",
    "    X_train: size m * n\n",
    "    Y_train: size 1 * n\n",
    "    '''\n",
    "    cost_hist = np.zeros((max_iter,1))\n",
    "    n_corr_hist = np.zeros((max_iter,1))\n",
    "    [W1, b1, W2, b2] = [params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "    for i in range(max_iter):\n",
    "        fprop_cache = fprop_logistic(X_train, Y_train, params)\n",
    "        n_correct = np.sum(np.round(fprop_cache['a2']) ==  Y_train)\n",
    "        cost_hist[i] = np.mean(fprop_cache['loss'])\n",
    "        n_corr_hist[i] = n_correct\n",
    "        if verbose:\n",
    "            print('At iteration {}, prediction is {} with {}/{} correct'.format(i,fprop_cache['a2'],n_correct, n))\n",
    "        for n in range(4):\n",
    "            # Stochastic gradient descent\n",
    "            X_train1 = np.zeros((2,1))\n",
    "            X_train1[:,0] = X_train[:,n]\n",
    "            fprop_cache = fprop_logistic(X_train1, Y_train[n], params)\n",
    "            bprop_cache = bprop_logistic(fprop_cache)\n",
    "            W1 = W1 - alpha*bprop_cache['W1']\n",
    "            b1 = b1 - alpha*bprop_cache['b1']\n",
    "            W2 = W2 - alpha*bprop_cache['W2']\n",
    "            b2 = b2 - alpha*bprop_cache['b2']\n",
    "        params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return cost_hist, n_corr_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net_logistic_vec(X_train, Y_train, params, max_iter = 200, alpha = 0.5, verbose = False):\n",
    "    '''\n",
    "    Vectorised logistic neural network training\n",
    "    X_train: size m * n\n",
    "    Y_train: size 1 * n\n",
    "    param: initialsed network\n",
    "    '''\n",
    "    cost_hist = np.zeros((max_iter,1))\n",
    "    n_corr_hist = np.zeros((max_iter,1))\n",
    "    # Vectorised gradient descent\n",
    "    for i in range(max_iter):\n",
    "        fprop_cache = fprop_logistic(X_train, Y_train, params)\n",
    "        n_correct = np.sum(np.round(fprop_cache['a2']) ==  Y_train)\n",
    "        cost_hist[i] = np.mean(fprop_cache['loss'])\n",
    "        n_corr_hist[i] = n_correct\n",
    "        if verbose:\n",
    "            print('At iteration {}, prediction is {} with {}/{} correct'.format(i,fprop_cache['a2'],n_correct, n))\n",
    "        bprop_cache = bprop_logistic_vec(fprop_cache)\n",
    "        [W1, b1, W2, b2] = [params[key] - alpha*params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "        params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return cost_hist, n_corr_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x118e22588>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1104b9f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = np.array([[-1,-1],\n",
    "                    [-1,1],\n",
    "                    [1,1],\n",
    "                    [1,-1]])\n",
    "X_train = X_train.T\n",
    "Y_train = np.array([0, 1, 0, 1])\n",
    "[m,n] = np.shape(X_train)\n",
    "params = initialise_param(m, n, n1 = 3, n2 = 1)\n",
    "cost_hist1, n_corr_hist1 = train_net_logistic_stochastic(X_train, Y_train, params, max_iter = 20, alpha = 0.1, verbose = False)\n",
    "cost_hist2, n_corr_hist2 = train_net_logistic_vec(X_train, Y_train, params, max_iter = 20, alpha = 0.1, verbose = False)\n",
    "plt.plot(cost_hist1)\n",
    "plt.plot(cost_hist2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network with softmax in the final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fprop(x, y, params):\n",
    "    W1, b1, W2, b2 = [params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "    z1 = np.dot(W1, x) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    loss = 0.5*np.linalg.norm(a2-y, axis = 0)**2\n",
    "    ret = {'x': x, 'y': y, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'loss': loss}\n",
    "    for key in params:\n",
    "        ret[key] = params[key]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bprop_vec(fprop_cache):\n",
    "  # Follows procedure given in notes\n",
    "    x, y, z1, a1, z2, a2, loss = [fprop_cache[key] for key in ('x', 'y', 'z1', 'a1', 'z2', 'a2', 'loss')]\n",
    "    delta2 = np.multiply(a2 - y, a2*(1-a2))\n",
    "    dW2 = np.dot(delta2, a1.T)\n",
    "    db2 = np.sum(delta2, 1)\n",
    "    # Why W2? Shouldn't it be W1??!!\n",
    "    delta1 = np.dot(fprop_cache['W2'].T, delta2) * a1 * (1-a1)\n",
    "    dW1 = np.dot(delta1, x.T)\n",
    "    db1 = np.sum(delta1,1)\n",
    "    return {'b1': db1, 'W1': dW1, 'b2': db2, 'W2': dW2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net_vec(X_train, Y_train, params, max_iter = 200, alpha = 0.5, verbose = False):\n",
    "    '''\n",
    "    Vectorised neural network training\n",
    "    X_train: size m * n\n",
    "    Y_train: size 1 * n\n",
    "    param: initialsed network\n",
    "    '''\n",
    "    cost_hist = np.zeros((max_iter,1))\n",
    "    n_corr_hist = np.zeros((max_iter,1))\n",
    "    if verbose:\n",
    "        print('At iteration {}, prediction is {} with {}/{} correct'.format(i,fprop_cache['a2'],n_correct, n))\n",
    "    for i in range(max_iter):\n",
    "        fprop_cache = fprop_logistic(X_train1, Y_train[n], params)\n",
    "        bprop_cache = bprop_logistic_vec(fprop_cache)\n",
    "        [W1, b1, W2, b2] = [params[key] - alpha*params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "        params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return cost_hist, n_corr_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression on MNIST\n",
    "\n",
    "Let us first use a logistic regression to descriminate between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "X = np.ndfromtxt('images.csv', delimiter=',')\n",
    "y = np.ndfromtxt(\"labels.csv\", delimiter=',', dtype=np.int8)\n",
    "n = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out only 0 and 1 and split data\n",
    "ind = np.logical_or(y == 1, y == 0)\n",
    "X = X[ind, :]\n",
    "y = y[ind]\n",
    "\n",
    "num_train = int(len(y) * 0.8)\n",
    "X_train = X[0:num_train, :].T\n",
    "X_test = X[num_train:-1,:].T\n",
    "y_train = y[0:num_train].T\n",
    "y_test = y[num_train:-1].T\n",
    "\n",
    "X_train = X_train/256\n",
    "X_test = X_test/256\n",
    "m = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
