{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network\n",
    "Meichen Lu (meichenlu91@gmail.com) 13rd April 2018\n",
    "\n",
    "In this notebook, I will implement a simple 2-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fprop_logistic(x, y, params):\n",
    "  # Follows procedure given in notes\n",
    "    W1, b1, W2, b2 = [params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "    z1 = np.dot(W1, x) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    loss = -(y * np.log(a2) + (1-y) * np.log(1-a2))\n",
    "    ret = {'x': x, 'y': y, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'loss': loss}\n",
    "    for key in params:\n",
    "        ret[key] = params[key]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bprop_logistic(fprop_cache):\n",
    "  # Follows procedure given in notes\n",
    "    x, y, z1, a1, z2, a2, loss = [fprop_cache[key] for key in ('x', 'y', 'z1', 'a1', 'z2', 'a2', 'loss')]\n",
    "    dz2 = (a2 - y)\n",
    "    dW2 = np.dot(dz2, a1.T)\n",
    "    db2 = dz2\n",
    "    dz1 = np.dot(fprop_cache['W2'].T, dz2) * sigmoid(z1) * (1-sigmoid(z1))\n",
    "    dW1 = np.dot(dz1, x.T)\n",
    "    db1 = dz1\n",
    "    return {'b1': db1, 'W1': dW1, 'b2': db2, 'W2': dW2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bprop_logistic_vec(fprop_cache):\n",
    "  # Vectorised version of the bprop_logistic()\n",
    "    x, y, z1, a1, z2, a2, loss = [fprop_cache[key] for key in ('x', 'y', 'z1', 'a1', 'z2', 'a2', 'loss')]\n",
    "    delta2 = (a2 - y)\n",
    "    dW2 = np.dot(delta2, a1.T)\n",
    "    db2 = np.sum(delta2, 1) # Add up contribution from all examples\n",
    "    delta1 = np.dot(fprop_cache['W2'].T, delta2) * sigmoid(z1) * (1-sigmoid(z1))\n",
    "    dW1 = np.dot(delta1, x.T)\n",
    "    db1 = np.sum(delta1,1)\n",
    "    return {'b1': db1, 'W1': dW1, 'b2': db2, 'W2': dW2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_gradient_bprop_logistic(m, n, params):\n",
    "    from copy import copy\n",
    "# Initialize random parameters and inputs\n",
    "    x = np.random.rand(m, n)\n",
    "    y = np.random.randint(0, 2, size=(1,n))  # Returns 0/1\n",
    "\n",
    "    fprop_cache = fprop_logistic(x, y, params)\n",
    "    if n == 1:\n",
    "        print('check stochastic gradient descent')\n",
    "        bprop_cache = bprop_logistic(fprop_cache)\n",
    "    else:\n",
    "        bprop_cache = bprop_logistic_vec(fprop_cache)\n",
    "    \n",
    "    # Numerical gradient checking\n",
    "    # Note how slow this is! Thus we want to use the backpropagation algorithm instead.\n",
    "    eps = 1e-6\n",
    "    ng_cache = {}\n",
    "    # For every single parameter (W, b)\n",
    "    for key in params:\n",
    "        param = params[key]\n",
    "        # This will be our numerical gradient\n",
    "        ng = np.zeros(param.shape)\n",
    "        for j in range(ng.shape[0]):\n",
    "            for k in range(ng.shape[1]):\n",
    "                # For every element of parameter matrix, compute gradient of loss wrt\n",
    "                # that element numerically using finite differences\n",
    "                add_eps = np.copy(param)\n",
    "                min_eps = np.copy(param)\n",
    "                add_eps[j, k] += eps\n",
    "                min_eps[j, k] -= eps\n",
    "                add_params = copy(params)\n",
    "                min_params = copy(params)\n",
    "                add_params[key] = add_eps\n",
    "                min_params[key] = min_eps\n",
    "                ng[j, k] = np.sum(fprop_logistic(x, y, add_params)['loss'] - fprop_logistic(x, y, min_params)['loss']) / (2 * eps)\n",
    "            ng_cache[key] = ng\n",
    "\n",
    "    # Compare numerical gradients to those computed using backpropagation algorithm\n",
    "    for key in params:\n",
    "        print(key)\n",
    "        # These should be the same\n",
    "        print(bprop_cache[key])\n",
    "        print(ng_cache[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialise_param(m, n, n1, n2):\n",
    "    '''\n",
    "    m features, n samples, first layer with n1 cells, second layer with n2 cells\n",
    "    '''\n",
    "    W1 = np.random.rand(n1, m)\n",
    "    b1 = np.random.rand(n1, 1)\n",
    "    W2 = np.random.rand(n2, n1)\n",
    "    b2 = np.random.rand(n2, 1)\n",
    "    params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check stochastic gradient descent\n",
      "W1\n",
      "[[ 0.05899921  0.01094255]\n",
      " [ 0.08199823  0.01520816]\n",
      " [ 0.06211878  0.01152113]]\n",
      "[[ 0.05899921  0.01094255]\n",
      " [ 0.08199823  0.01520816]\n",
      " [ 0.06211878  0.01152113]]\n",
      "b1\n",
      "[[ 0.0984633 ]\n",
      " [ 0.13684618]\n",
      " [ 0.10366952]]\n",
      "[[ 0.0984633 ]\n",
      " [ 0.13684618]\n",
      " [ 0.10366952]]\n",
      "W2\n",
      "[[ 0.61362244  0.53093868  0.58239993]]\n",
      "[[ 0.61362244  0.53093868  0.58239993]]\n",
      "b2\n",
      "[[ 0.83940254]]\n",
      "[[ 0.83940254]]\n"
     ]
    }
   ],
   "source": [
    "m = 2 # features\n",
    "n = 1 # examples\n",
    "params = initialise_param(m, n, n1 = 3, n2 = 1)\n",
    "# Stochastic case\n",
    "numerical_gradient_bprop_logistic(m, n, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1\n",
      "[[ 0.08746158  0.01376536]\n",
      " [ 0.07796684  0.01230863]\n",
      " [ 0.03607751  0.00569313]]\n",
      "[[ 0.08746158  0.01376536]\n",
      " [ 0.07796684  0.01230863]\n",
      " [ 0.03607751  0.00569313]]\n",
      "b1\n",
      "[ 0.15720113  0.13856309  0.06421861]\n",
      "[[ 0.15720113]\n",
      " [ 0.13856309]\n",
      " [ 0.06421861]]\n",
      "W2\n",
      "[[ 1.07937544  0.99306597  1.05064755]]\n",
      "[[ 1.07937544  0.99306597  1.05064755]]\n",
      "b2\n",
      "[ 1.46690793]\n",
      "[[ 1.46690793]]\n"
     ]
    }
   ],
   "source": [
    "m = 2 # features\n",
    "n = 2 # examples\n",
    "params = initialise_param(m, n, n1 = 3, n2 = 1)\n",
    "# Vectorised case\n",
    "numerical_gradient_bprop_logistic(m, n, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net_logistic_stochastic(X_train, Y_train, params, max_iter = 200, alpha = 0.5, verbose = False):\n",
    "    '''\n",
    "    X_train: size m * n\n",
    "    Y_train: size 1 * n\n",
    "    '''\n",
    "    cost_hist = np.zeros((max_iter,1))\n",
    "    n_corr_hist = np.zeros((max_iter,1))\n",
    "    W1 = np.array([[0.2, 0.3], [0.4,0.3]])\n",
    "    b1 = np.zeros((2,1))\n",
    "    b1[0] = 0.3\n",
    "    b1[1] = 0.2\n",
    "    W2 = np.zeros((2,1))\n",
    "    W2[0] = 0.4\n",
    "    W2[1] = 0.6\n",
    "    params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': np.array([0.2])}\n",
    "    [W1, b1, W2, b2] = [params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "    for i in range(max_iter):\n",
    "        fprop_cache = fprop_logistic(X_train, Y_train, params)\n",
    "        n_correct = np.sum(np.round(fprop_cache['a2']) ==  Y_train)\n",
    "        cost_hist[i] = np.mean(fprop_cache['loss'])\n",
    "        n_corr_hist[i] = n_correct\n",
    "        if verbose:\n",
    "            print('At iteration {}, prediction is {} with {}/{} correct'.format(i,fprop_cache['a2'],n_correct, n))\n",
    "        for n in range(4):\n",
    "            # Stochastic gradient descent\n",
    "            X_train1 = np.zeros((2,1))\n",
    "            X_train1[:,0] = X_train[:,n]\n",
    "            fprop_cache = fprop_logistic(X_train1, Y_train[n], params)\n",
    "            bprop_cache = bprop_logistic(fprop_cache)\n",
    "            W1 = W1 - alpha*bprop_cache['W1']\n",
    "            b1 = b1 - alpha*bprop_cache['b1']\n",
    "            W2 = W2 - alpha*bprop_cache['W2']\n",
    "            b2 = b2 - alpha*bprop_cache['b2']\n",
    "        params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        print(params)\n",
    "    return cost_hist, n_corr_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net_logistic_vec(X_train, Y_train, params, max_iter = 200, alpha = 0.5, verbose = False):\n",
    "    '''\n",
    "    Vectorised logistic neural network training\n",
    "    X_train: size m * n\n",
    "    Y_train: size 1 * n\n",
    "    param: initialsed network\n",
    "    '''\n",
    "    cost_hist = np.zeros((max_iter,1))\n",
    "    n_corr_hist = np.zeros((max_iter,1))\n",
    "    W1 = np.array([[0.2, 0.3], [0.4,0.3]])\n",
    "    b1 = np.zeros((2,1))\n",
    "    b1[0] = 0.3\n",
    "    b1[1] = 0.2\n",
    "    W2 = np.zeros((2,1))\n",
    "    W2[0] = 0.4\n",
    "    W2[1] = 0.6\n",
    "    params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': np.array([0.2])}\n",
    "    # Vectorised gradient descent\n",
    "    for i in range(max_iter):\n",
    "        fprop_cache = fprop_logistic(X_train, Y_train, params)\n",
    "        n_correct = np.sum(np.round(fprop_cache['a2']) ==  Y_train)\n",
    "        cost_hist[i] = np.mean(fprop_cache['loss'])\n",
    "        n_corr_hist[i] = n_correct\n",
    "        if verbose:\n",
    "            print('At iteration {}, prediction is {} with {}/{} correct'.format(i,fprop_cache['a2'],n_correct, n))\n",
    "        bprop_cache = bprop_logistic_vec(fprop_cache)\n",
    "        [W1, b1, W2, b2] = [params[key] - alpha*params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "        params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return cost_hist, n_corr_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,1) and (2,4) not aligned: 1 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e9946287373d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialise_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcost_hist1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_corr_hist1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_net_logistic_stochastic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mcost_hist2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_corr_hist2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_net_logistic_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_hist1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-642ab4f79df8>\u001b[0m in \u001b[0;36mtrain_net_logistic_stochastic\u001b[0;34m(X_train, Y_train, params, max_iter, alpha, verbose)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'W2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mfprop_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfprop_logistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mn_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfprop_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m  \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcost_hist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfprop_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c36a1903f045>\u001b[0m in \u001b[0;36mfprop_logistic\u001b[0;34m(x, y, params)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mz2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,1) and (2,4) not aligned: 1 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[-1,-1],\n",
    "                    [-1,1],\n",
    "                    [1,1],\n",
    "                    [1,-1]])\n",
    "X_train = X_train.T\n",
    "Y_train = np.array([0, 1, 0, 1])\n",
    "[m,n] = np.shape(X_train)\n",
    "params = initialise_param(m, n, n1 = 2, n2 = 1)\n",
    "cost_hist1, n_corr_hist1 = train_net_logistic_stochastic(X_train, Y_train, params, max_iter = 1, alpha = 0.1, verbose = False)\n",
    "cost_hist2, n_corr_hist2 = train_net_logistic_vec(X_train, Y_train, params, max_iter = 1, alpha = 0.1, verbose = False)\n",
    "plt.plot(cost_hist1)\n",
    "plt.plot(cost_hist2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network with softmax in the final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fprop(x, y, params):\n",
    "    W1, b1, W2, b2 = [params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "    z1 = np.dot(W1, x) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    loss = 0.5*np.linalg.norm(a2-y, axis = 0)**2\n",
    "    ret = {'x': x, 'y': y, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'loss': loss}\n",
    "    for key in params:\n",
    "        ret[key] = params[key]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bprop_vec(fprop_cache):\n",
    "  # Follows procedure given in notes\n",
    "    x, y, z1, a1, z2, a2, loss = [fprop_cache[key] for key in ('x', 'y', 'z1', 'a1', 'z2', 'a2', 'loss')]\n",
    "    delta2 = np.multiply(a2 - y, a2*(1-a2))\n",
    "    dW2 = np.dot(delta2, a1.T)\n",
    "    db2 = np.sum(delta2, 1)\n",
    "    # Why W2? Shouldn't it be W1??!!\n",
    "    delta1 = np.dot(fprop_cache['W2'].T, delta2) * a1 * (1-a1)\n",
    "    dW1 = np.dot(delta1, x.T)\n",
    "    db1 = np.sum(delta1,1)\n",
    "    return {'b1': db1, 'W1': dW1, 'b2': db2, 'W2': dW2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net_vec(X_train, Y_train, params, max_iter = 200, alpha = 0.5, verbose = False):\n",
    "    '''\n",
    "    Vectorised neural network training\n",
    "    X_train: size m * n\n",
    "    Y_train: size 1 * n\n",
    "    param: initialsed network\n",
    "    '''\n",
    "    cost_hist = np.zeros((max_iter,1))\n",
    "    n_corr_hist = np.zeros((max_iter,1))\n",
    "    if verbose:\n",
    "        print('At iteration {}, prediction is {} with {}/{} correct'.format(i,fprop_cache['a2'],n_correct, n))\n",
    "    for i in range(max_iter):\n",
    "        fprop_cache = fprop_logistic(X_train1, Y_train[n], params)\n",
    "        bprop_cache = bprop_logistic_vec(fprop_cache)\n",
    "        [W1, b1, W2, b2] = [params[key] - alpha*params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "        params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return cost_hist, n_corr_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression on MNIST\n",
    "\n",
    "Let us first use a logistic regression to descriminate between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "X = np.ndfromtxt('images.csv', delimiter=',')\n",
    "y = np.ndfromtxt(\"labels.csv\", delimiter=',', dtype=np.int8)\n",
    "n = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out only 0 and 1 and split data\n",
    "ind = np.logical_or(y == 1, y == 0)\n",
    "X = X[ind, :]\n",
    "y = y[ind]\n",
    "\n",
    "num_train = int(len(y) * 0.8)\n",
    "X_train = X[0:num_train, :].T\n",
    "X_test = X[num_train:-1,:].T\n",
    "y_train = y[0:num_train].T\n",
    "y_test = y[num_train:-1].T\n",
    "\n",
    "X_train = X_train/256\n",
    "X_test = X_test/256\n",
    "m = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
