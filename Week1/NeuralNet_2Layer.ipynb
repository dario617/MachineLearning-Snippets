{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network\n",
    "Meichen Lu (meichenlu91@gmail.com) 13rd April 2018\n",
    "\n",
    "In this notebook, I will implement a simple 2-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fprop_logistic(x, y, params):\n",
    "  # Follows procedure given in notes\n",
    "    W1, b1, W2, b2 = [params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "    z1 = np.dot(W1, x) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    loss = -(y * np.log(a2) + (1-y) * np.log(1-a2))\n",
    "    ret = {'x': x, 'y': y, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'loss': loss}\n",
    "    for key in params:\n",
    "        ret[key] = params[key]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bprop_logistic(fprop_cache):\n",
    "  # Follows procedure given in notes\n",
    "    x, y, z1, a1, z2, a2, loss = [fprop_cache[key] for key in ('x', 'y', 'z1', 'a1', 'z2', 'a2', 'loss')]\n",
    "    dz2 = (a2 - y)\n",
    "    dW2 = np.dot(dz2, a1.T)\n",
    "    db2 = dz2\n",
    "    dz1 = np.dot(fprop_cache['W2'].T, dz2) * sigmoid(z1) * (1-sigmoid(z1))\n",
    "    dW1 = np.dot(dz1, x.T)\n",
    "    db1 = dz1\n",
    "    return {'b1': db1, 'W1': dW1, 'b2': db2, 'W2': dW2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bprop_logistic_vec(fprop_cache):\n",
    "  # Vectorised version of the bprop_logistic()\n",
    "    x, y, z1, a1, z2, a2, loss = [fprop_cache[key] for key in ('x', 'y', 'z1', 'a1', 'z2', 'a2', 'loss')]\n",
    "    delta2 = (a2 - y)\n",
    "    dW2 = np.dot(delta2, a1.T)\n",
    "    db2 = np.sum(delta2, 1) # Add up contribution from all examples\n",
    "    delta1 = np.dot(fprop_cache['W2'].T, delta2) * sigmoid(z1) * (1-sigmoid(z1))\n",
    "    dW1 = np.dot(delta1, x.T)\n",
    "    db1 = np.sum(delta1,1)\n",
    "    return {'b1': db1, 'W1': dW1, 'b2': db2, 'W2': dW2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_gradient_bprop_logistic(m, n, params):\n",
    "    from copy import copy\n",
    "# Initialize random parameters and inputs\n",
    "    x = np.random.rand(m, n)\n",
    "    y = np.random.randint(0, 2, size=(1,n))  # Returns 0/1\n",
    "\n",
    "    fprop_cache = fprop_logistic(x, y, params)\n",
    "    if n == 1:\n",
    "        print('check stochastic gradient descent')\n",
    "        bprop_cache = bprop_logistic(fprop_cache)\n",
    "    else:\n",
    "        bprop_cache = bprop_logistic_vec(fprop_cache)\n",
    "    \n",
    "    # Numerical gradient checking\n",
    "    # Note how slow this is! Thus we want to use the backpropagation algorithm instead.\n",
    "    eps = 1e-6\n",
    "    ng_cache = {}\n",
    "    # For every single parameter (W, b)\n",
    "    for key in params:\n",
    "        param = params[key]\n",
    "        # This will be our numerical gradient\n",
    "        ng = np.zeros(param.shape)\n",
    "        for j in range(ng.shape[0]):\n",
    "            for k in range(ng.shape[1]):\n",
    "                # For every element of parameter matrix, compute gradient of loss wrt\n",
    "                # that element numerically using finite differences\n",
    "                add_eps = np.copy(param)\n",
    "                min_eps = np.copy(param)\n",
    "                add_eps[j, k] += eps\n",
    "                min_eps[j, k] -= eps\n",
    "                add_params = copy(params)\n",
    "                min_params = copy(params)\n",
    "                add_params[key] = add_eps\n",
    "                min_params[key] = min_eps\n",
    "                ng[j, k] = np.sum(fprop_logistic(x, y, add_params)['loss'] - fprop_logistic(x, y, min_params)['loss']) / (2 * eps)\n",
    "            ng_cache[key] = ng\n",
    "\n",
    "    # Compare numerical gradients to those computed using backpropagation algorithm\n",
    "    for key in params:\n",
    "        print(key)\n",
    "        # These should be the same\n",
    "        print(bprop_cache[key])\n",
    "        print(ng_cache[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialise_param(m, n, n1, n2):\n",
    "    '''\n",
    "    m features, n samples, first layer with n1 cells, second layer with n2 cells\n",
    "    '''\n",
    "    W1 = np.random.rand(n1, m)\n",
    "    b1 = np.random.rand(n1, 1)\n",
    "    W2 = np.random.rand(n2, n1)\n",
    "    b2 = np.random.rand(n2, 1)\n",
    "    params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check stochastic gradient descent\n",
      "W1\n",
      "[[-0.01044944 -0.00905243]\n",
      " [-0.02119774 -0.01836377]\n",
      " [-0.02314047 -0.02004677]]\n",
      "[[-0.01044944 -0.00905243]\n",
      " [-0.02119774 -0.01836377]\n",
      " [-0.02314047 -0.02004677]]\n",
      "b1\n",
      "[[-0.01053762]\n",
      " [-0.02137662]\n",
      " [-0.02333575]]\n",
      "[[-0.01053762]\n",
      " [-0.02137662]\n",
      " [-0.02333575]]\n",
      "W2\n",
      "[[-0.14064194 -0.12525921 -0.12178379]]\n",
      "[[-0.14064194 -0.12525921 -0.12178379]]\n",
      "b2\n",
      "[[-0.16449282]]\n",
      "[[-0.16449282]]\n"
     ]
    }
   ],
   "source": [
    "m = 2 # features\n",
    "n = 1 # examples\n",
    "params = initialise_param(m, n, n1 = 3, n2 = 1)\n",
    "# Stochastic case\n",
    "numerical_gradient_bprop_logistic(m, n, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1\n",
      "[[ 0.01248834  0.03139929]\n",
      " [ 0.02002101  0.05116814]\n",
      " [ 0.02322071  0.0591801 ]]\n",
      "[[ 0.01248834  0.03139929]\n",
      " [ 0.02002101  0.05116814]\n",
      " [ 0.02322071  0.0591801 ]]\n",
      "b1\n",
      "[ 0.04633738  0.0753169   0.08714826]\n",
      "[[ 0.04633738]\n",
      " [ 0.0753169 ]\n",
      " [ 0.08714826]]\n",
      "W2\n",
      "[[ 0.4313243   0.43120295  0.33116453]]\n",
      "[[ 0.4313243   0.43120295  0.33116453]]\n",
      "b2\n",
      "[ 0.57304144]\n",
      "[[ 0.57304144]]\n"
     ]
    }
   ],
   "source": [
    "m = 2 # features\n",
    "n = 2 # examples\n",
    "params = initialise_param(m, n, n1 = 3, n2 = 1)\n",
    "# Vectorised case\n",
    "numerical_gradient_bprop_logistic(m, n, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net_logistic_stochastic(X_train, Y_train, params, max_iter = 200, alpha = 0.5, verbose = False):\n",
    "    '''\n",
    "    X_train: size m * n\n",
    "    Y_train: size 1 * n\n",
    "    '''\n",
    "    cost_hist = np.zeros((max_iter,1))\n",
    "    n_corr_hist = np.zeros((max_iter,1))\n",
    "    [W1, b1, W2, b2] = [params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "    for i in range(max_iter):\n",
    "        fprop_cache = fprop_logistic(X_train, Y_train, params)\n",
    "        n_correct = np.sum(np.round(fprop_cache['a2']) ==  Y_train)\n",
    "        cost_hist[i] = np.mean(fprop_cache['loss'])\n",
    "        n_corr_hist[i] = n_correct\n",
    "        if verbose:\n",
    "            print('At iteration {}, prediction is {} with {}/{} correct'.format(i,fprop_cache['a2'],n_correct, n))\n",
    "        for n in range(4):\n",
    "            # Stochastic gradient descent\n",
    "            X_train1 = np.zeros((2,1))\n",
    "            X_train1[:,0] = X_train[:,n]\n",
    "            fprop_cache = fprop_logistic(X_train1, Y_train[n], params)\n",
    "            bprop_cache = bprop_logistic(fprop_cache)\n",
    "            W1 = W1 - alpha*bprop_cache['W1']\n",
    "            b1 = b1 - alpha*bprop_cache['b1']\n",
    "            W2 = W2 - alpha*bprop_cache['W2']\n",
    "            b2 = b2 - alpha*bprop_cache['b2']\n",
    "        params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return cost_hist, n_corr_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net_logistic_vec(X_train, Y_train, params, max_iter = 200, alpha = 0.5, verbose = False):\n",
    "    '''\n",
    "    Vectorised logistic neural network training\n",
    "    X_train: size m * n\n",
    "    Y_train: size 1 * n\n",
    "    param: initialsed network\n",
    "    '''\n",
    "    cost_hist = np.zeros((max_iter,1))\n",
    "    n_corr_hist = np.zeros((max_iter,1))\n",
    "    # Vectorised gradient descent\n",
    "    for i in range(max_iter):\n",
    "        fprop_cache = fprop_logistic(X_train, Y_train, params)\n",
    "        n_correct = np.sum(np.round(fprop_cache['a2']) ==  Y_train)\n",
    "        cost_hist[i] = np.mean(fprop_cache['loss'])\n",
    "        n_corr_hist[i] = n_correct\n",
    "        if verbose:\n",
    "            print('At iteration {}, prediction is {} with {}/{} correct'.format(i,fprop_cache['a2'],n_correct, n))\n",
    "        bprop_cache = bprop_logistic_vec(fprop_cache)\n",
    "        [W1, b1, W2, b2] = [params[key] - alpha*params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "        params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return cost_hist, n_corr_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x121ad6b00>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121427d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = np.array([[-1,-1],\n",
    "                    [-1,1],\n",
    "                    [1,1],\n",
    "                    [1,-1]])\n",
    "X_train = X_train.T\n",
    "Y_train = np.array([0, 1, 0, 1])\n",
    "[m,n] = np.shape(X_train)\n",
    "params = initialise_param(m, n, n1 = 3, n2 = 1)\n",
    "cost_hist1, n_corr_hist1 = train_net_logistic_stochastic(X_train, Y_train, params, max_iter = 20, alpha = 0.1, verbose = False)\n",
    "cost_hist2, n_corr_hist2 = train_net_logistic_vec(X_train, Y_train, params, max_iter = 20, alpha = 0.1, verbose = False)\n",
    "plt.plot(cost_hist1)\n",
    "plt.plot(cost_hist2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network with softmax in the final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fprop(x, y, params):\n",
    "    W1, b1, W2, b2 = [params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "    z1 = np.dot(W1, x) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    loss = 0.5*np.linalg.norm(a2-y, axis = 0)**2\n",
    "    ret = {'x': x, 'y': y, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'loss': loss}\n",
    "    for key in params:\n",
    "        ret[key] = params[key]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bprop_vec(fprop_cache):\n",
    "  # Follows procedure given in notes\n",
    "    x, y, z1, a1, z2, a2, loss = [fprop_cache[key] for key in ('x', 'y', 'z1', 'a1', 'z2', 'a2', 'loss')]\n",
    "    delta2 = np.multiply(a2 - y, a2*(1-a2))\n",
    "    dW2 = np.dot(delta2, a1.T)\n",
    "    db2 = np.sum(delta2, 1)\n",
    "    # Why W2? Shouldn't it be W1??!!\n",
    "    delta1 = np.dot(fprop_cache['W2'].T, delta2) * a1 * (1-a1)\n",
    "    dW1 = np.dot(delta1, x.T)\n",
    "    db1 = np.sum(delta1,1)\n",
    "    return {'b1': db1, 'W1': dW1, 'b2': db2, 'W2': dW2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net_vec(X_train, Y_train, params, max_iter = 200, alpha = 0.5, verbose = False):\n",
    "    '''\n",
    "    Vectorised neural network training\n",
    "    X_train: size m * n\n",
    "    Y_train: size 1 * n\n",
    "    param: initialsed network\n",
    "    '''\n",
    "    cost_hist = np.zeros((max_iter,1))\n",
    "    n_corr_hist = np.zeros((max_iter,1))\n",
    "    if verbose:\n",
    "        print('At iteration {}, prediction is {} with {}/{} correct'.format(i,fprop_cache['a2'],n_correct, n))\n",
    "    for i in range(max_iter):\n",
    "        fprop_cache = fprop_logistic(X_train1, Y_train[n], params)\n",
    "        bprop_cache = bprop_logistic_vec(fprop_cache)\n",
    "        [W1, b1, W2, b2] = [params[key] - alpha*params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "        params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return cost_hist, n_corr_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression on MNIST\n",
    "\n",
    "Let us first use a logistic regression to descriminate between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "X = np.ndfromtxt('images.csv', delimiter=',')\n",
    "y = np.ndfromtxt(\"labels.csv\", delimiter=',', dtype=np.int8)\n",
    "n = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out only 0 and 1 and split data\n",
    "ind = np.logical_or(y == 1, y == 0)\n",
    "X = X[ind, :]\n",
    "y = y[ind]\n",
    "\n",
    "num_train = int(len(y) * 0.8)\n",
    "X_train = X[0:num_train, :].T\n",
    "X_test = X[num_train:-1,:].T\n",
    "y_train = y[0:num_train].T\n",
    "y_test = y[num_train:-1].T\n",
    "\n",
    "X_train = X_train/256\n",
    "X_test = X_test/256\n",
    "m = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
